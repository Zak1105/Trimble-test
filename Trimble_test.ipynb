{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zak1105/Trimble-test/blob/main/Trimble_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eGTsApKoHrY",
        "outputId": "24c7cda8-955c-4ff2-f7da-0eccae4b6eb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dataset.zip\n",
            "replace dataset/fields/1.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: dataset/fields/1.jpg    \n",
            "  inflating: dataset/fields/10.jpg   \n",
            "  inflating: dataset/fields/11.jpg   \n",
            "  inflating: dataset/fields/12.jpg   \n",
            "  inflating: dataset/fields/13.jpg   \n",
            "  inflating: dataset/fields/14.jpg   \n",
            "  inflating: dataset/fields/15.jpg   \n",
            "  inflating: dataset/fields/16.jpg   \n",
            "  inflating: dataset/fields/17.jpg   \n",
            "  inflating: dataset/fields/18.jpg   \n",
            "  inflating: dataset/fields/19.jpg   \n",
            "  inflating: dataset/fields/2.jpg    \n",
            "  inflating: dataset/fields/20.jpg   \n",
            "  inflating: dataset/fields/21.jpg   \n",
            "  inflating: dataset/fields/22.jpg   \n",
            "  inflating: dataset/fields/23.jpg   \n",
            "  inflating: dataset/fields/24.jpg   \n",
            "  inflating: dataset/fields/25.jpg   \n",
            "  inflating: dataset/fields/26.jpg   \n",
            "  inflating: dataset/fields/27.jpg   \n",
            "  inflating: dataset/fields/28.jpg   \n",
            "  inflating: dataset/fields/29.jpg   \n",
            "  inflating: dataset/fields/3.jpg    \n",
            "  inflating: dataset/fields/30.jpg   \n",
            "  inflating: dataset/fields/31.jpg   \n",
            "  inflating: dataset/fields/32.jpg   \n",
            "  inflating: dataset/fields/33.jpg   \n",
            "  inflating: dataset/fields/34.jpg   \n",
            "  inflating: dataset/fields/35.jpg   \n",
            "  inflating: dataset/fields/36.jpg   \n",
            "  inflating: dataset/fields/37.jpg   \n",
            "  inflating: dataset/fields/38.jpg   \n",
            "  inflating: dataset/fields/39.jpg   \n",
            "  inflating: dataset/fields/4.jpg    \n",
            "  inflating: dataset/fields/40.jpg   \n",
            "  inflating: dataset/fields/41.jpg   \n",
            "  inflating: dataset/fields/42.jpg   \n",
            "  inflating: dataset/fields/43.jpg   \n",
            "  inflating: dataset/fields/44.jpg   \n",
            "  inflating: dataset/fields/45.jpg   \n",
            "  inflating: dataset/fields/5.jpg    \n",
            "  inflating: dataset/fields/6.jpg    \n",
            "  inflating: dataset/fields/7.jpg    \n",
            "  inflating: dataset/fields/8.jpg    \n",
            "  inflating: dataset/fields/9.jpg    \n",
            "  inflating: dataset/roads/1.jpg     \n",
            "  inflating: dataset/roads/10.jpg    \n",
            "  inflating: dataset/roads/11.jpg    \n",
            "  inflating: dataset/roads/12.jpg    \n",
            "  inflating: dataset/roads/13.jpg    \n",
            "  inflating: dataset/roads/14.jpg    \n",
            "  inflating: dataset/roads/15.jpg    \n",
            "  inflating: dataset/roads/16.jpg    \n",
            "  inflating: dataset/roads/17.jpg    \n",
            "  inflating: dataset/roads/18.jpg    \n",
            "  inflating: dataset/roads/19.jpg    \n",
            "  inflating: dataset/roads/2.jpg     \n",
            "  inflating: dataset/roads/20.jpg    \n",
            "  inflating: dataset/roads/21.jpg    \n",
            "  inflating: dataset/roads/22.jpg    \n",
            "  inflating: dataset/roads/23.jpg    \n",
            "  inflating: dataset/roads/24.jpg    \n",
            "  inflating: dataset/roads/25.jpg    \n",
            "  inflating: dataset/roads/26.jpg    \n",
            "  inflating: dataset/roads/27.jpg    \n",
            "  inflating: dataset/roads/28.jpg    \n",
            "  inflating: dataset/roads/29.jpg    \n",
            "  inflating: dataset/roads/3.jpg     \n",
            "  inflating: dataset/roads/30.jpg    \n",
            "  inflating: dataset/roads/31.jpg    \n",
            "  inflating: dataset/roads/32.jpg    \n",
            "  inflating: dataset/roads/33.jpg    \n",
            "  inflating: dataset/roads/34.jpg    \n",
            "  inflating: dataset/roads/35.jpg    \n",
            "  inflating: dataset/roads/36.jpg    \n",
            "  inflating: dataset/roads/37.jpg    \n",
            "  inflating: dataset/roads/38.jpg    \n",
            "  inflating: dataset/roads/39.jpg    \n",
            "  inflating: dataset/roads/4.jpg     \n",
            "  inflating: dataset/roads/40.jpg    \n",
            "  inflating: dataset/roads/41.jpg    \n",
            "  inflating: dataset/roads/42.jpg    \n",
            "  inflating: dataset/roads/43.jpg    \n",
            "  inflating: dataset/roads/44.jpg    \n",
            "  inflating: dataset/roads/45.jpg    \n",
            "  inflating: dataset/roads/46.jpeg   \n",
            "  inflating: dataset/roads/47.jpeg   \n",
            "  inflating: dataset/roads/48.jpeg   \n",
            "  inflating: dataset/roads/49.jpeg   \n",
            "  inflating: dataset/roads/5.jpg     \n",
            "  inflating: dataset/roads/50.jpeg   \n",
            "  inflating: dataset/roads/51.jpeg   \n",
            "  inflating: dataset/roads/52.jpeg   \n",
            "  inflating: dataset/roads/53.jpeg   \n",
            "  inflating: dataset/roads/6.jpg     \n",
            "  inflating: dataset/roads/7.jpg     \n",
            "  inflating: dataset/roads/8.jpg     \n",
            "  inflating: dataset/roads/9.jpg     \n",
            "  inflating: dataset/roads/city-6238228__340.jpg  \n",
            "  inflating: dataset/roads/crops-1835847__340.jpg  \n",
            "  inflating: dataset/roads/death-valley-4250244__340.jpg  \n",
            "  inflating: dataset/roads/fog-6696312__340.jpg  \n",
            "  inflating: dataset/roads/forest-1835019__340.jpg  \n",
            "  inflating: dataset/roads/iceland-4957449__340.jpg  \n",
            "  inflating: dataset/roads/istockphoto-490608724-612x612.jpg  \n",
            "  inflating: dataset/roads/istockphoto-972849064-612x612.jpg  \n",
            "  inflating: dataset/roads/pexels-photo-1038935.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-1094545.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-1197095.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-125510.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-1374295.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-1471117.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-1496372.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-1590051.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-1808329.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-1955134.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-197900.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-209652.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-229014.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-2364401.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-2568025.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-2682122.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-2715004.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-2739010.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-2990770.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-3014002.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-3041347.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-3182530.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-3593923.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-3605316.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-3876397.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-39811.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-428431.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-57652.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-5898609.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-59512.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-672597.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-730662.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-756861.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-775199.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-775203.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-814667.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-844167.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-868677.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-935484.jpeg  \n",
            "  inflating: dataset/roads/pexels-photo-954276.jpeg  \n",
            "  inflating: dataset/roads/road-1303617__340.jpg  \n",
            "  inflating: dataset/roads/road-3186188__340.jpg  \n",
            "  inflating: dataset/roads/road-6281973__340.jpg  \n",
            "  inflating: dataset/roads/road-6597404__340.jpg  \n",
            "  inflating: dataset/roads/trees-sunrise-3796183__340.jpg  \n",
            "  inflating: dataset/roads/tunnel-6723643__340.jpg  \n",
            "  inflating: dataset/roads/winding-road-1556177__340.jpg  \n"
          ]
        }
      ],
      "source": [
        "!unzip dataset.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s09YHYCBog8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca7ffff4-de24-47bc-c54e-0a8d8d2e8bd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "---------------------------------- Start of training ----------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:03<00:00,  2.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 9.0000e-05.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.2917, Validation Loss: 0.0692, Train Accuracy: 90.98%, Validation Accuracy: 93.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:03<00:00,  2.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 8.1000e-05.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Train Loss: 0.0486, Validation Loss: 0.0220, Train Accuracy: 99.18%, Validation Accuracy: 96.77%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:03<00:00,  2.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 7.2900e-05.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Train Loss: 0.0116, Validation Loss: 0.0198, Train Accuracy: 100.00%, Validation Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:04<00:00,  1.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 6.5610e-05.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Train Loss: 0.0125, Validation Loss: 0.0282, Train Accuracy: 100.00%, Validation Accuracy: 96.77%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:03<00:00,  2.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 5.9049e-05.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Train Loss: 0.0054, Validation Loss: 0.0118, Train Accuracy: 100.00%, Validation Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:03<00:00,  2.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 5.3144e-05.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Train Loss: 0.0048, Validation Loss: 0.0308, Train Accuracy: 100.00%, Validation Accuracy: 96.77%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]Exception ignored in: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "# Define a transformation to resize images to 224x224 and perform data augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the data from the dataset folder\n",
        "dataset = ImageFolder(\"dataset\", transform=transform)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\"\"\"\n",
        "# Count the number of samples in each class\n",
        "\n",
        "count_class_0 = sum(labels == 0 for _, labels in train_dataset)\n",
        "count_class_1 = len(train_dataset) - count_class_0\n",
        "print(count_class_0)\n",
        "print(count_class_1)\n",
        "# Calculate the upsampling factor\n",
        "\n",
        "undersampled_train_dataset= []\n",
        "# Échantillons aléatoires de la classe majoritaire (classe 1)\n",
        "for inputs, labels in train_dataset:\n",
        "    if labels == 1 and random.random() < (count_class_0 / count_class_1):\n",
        "        undersampled_train_dataset.append((inputs, labels))\"\"\"\n",
        "batch_size=16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,num_workers=4)\n",
        "\n",
        "# Define the model ResNet101 with an output for classification\n",
        "class ResNetModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNetModel, self).__init__()\n",
        "        self.resnet = torch.hub.load('pytorch/vision', 'resnet101', pretrained=True)\n",
        "        num_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(num_features, 1)  # Output layer with one outputs\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        return self.sigmoid(self.resnet(x))\n",
        "\n",
        "# Instantiate the model\n",
        "model = ResNetModel()\n",
        "\n",
        "device =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion =nn.L1Loss().to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001,weight_decay=0.002)\n",
        "# Define th scheduler for optimisate the learning rate\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.9,verbose=True)\n",
        "# Training the model\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "print (\"---------------------------------- Start of training ----------------------------------\")\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train_samples = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        labels=labels.to(device)\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.unsqueeze(1).float() #\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        outputs[outputs>0.5]=1\n",
        "        outputs[outputs<0.5]=0\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        correct_train += (outputs == labels).sum().item()\n",
        "        total_train_samples += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct_train /total_train_samples\n",
        "    scheduler.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            labels=labels.to(device)\n",
        "            inputs, labels = inputs.to(device), labels.unsqueeze(1).float()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            outputs[outputs>0.5]=1\n",
        "            outputs[outputs<0.5]=0\n",
        "\n",
        "            correct_val += (outputs == labels).sum().item()\n",
        "\n",
        "    val_loss = running_loss / len(val_loader)\n",
        "    val_accuracy = 100 * correct_val / len(val_dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
        "          f\"Train Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "print(\"---------------------------------- End of trainning ----------------------------------\")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, epochs + 1), train_losses, label='Train')\n",
        "plt.plot(range(1, epochs + 1), val_losses, label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Train and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, epochs + 1), train_accuracies, label='Train')\n",
        "plt.plot(range(1, epochs + 1), val_accuracies, label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Train and Validation Accuracy')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/test.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZD_9NatCejM",
        "outputId": "8823095e-a2a3-47b1-9dcf-f4e380e629d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/test.zip\n",
            "replace test/test_images/1.jpeg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: test/test_images/1.jpeg  \n",
            "  inflating: test/test_images/10.jpeg  \n",
            "  inflating: test/test_images/2.jpeg  \n",
            "  inflating: test/test_images/3.jpeg  \n",
            "  inflating: test/test_images/4.jpeg  \n",
            "  inflating: test/test_images/5.jpeg  \n",
            "  inflating: test/test_images/6.jpeg  \n",
            "  inflating: test/test_images/7.jpeg  \n",
            "  inflating: test/test_images/8.jpeg  \n",
            "  inflating: test/test_images/9.jpeg  \n",
            "  inflating: plottt.py               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdiPiL8uBO5j"
      },
      "outputs": [],
      "source": [
        "torch.save(model,'model_final.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model.eval()\n",
        "test_dataset = ImageFolder(\"test\", transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
        "\n",
        "# Create prediction folders\n",
        "if not os.path.exists(\"predicted_0\"):\n",
        "    os.makedirs(\"predicted_0\")\n",
        "if not os.path.exists(\"predicted_1\"):\n",
        "    os.makedirs(\"predicted_1\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, (inputs, _) in enumerate(test_loader):\n",
        "        inputs =  inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predicted = 0 if outputs.item() < 0.5 else 1\n",
        "        print(f\"Image {idx}: Predicted {'road' if predicted == 1 else 'field'}\")\n",
        "\n",
        "        original_image = inputs.permute(0, 2, 3, 1).cpu().numpy().squeeze()\n",
        "\n",
        "        # Save the image with appropriate naming\n",
        "        prediction_folder = \"predicted_1\" if predicted == 1 else \"predicted_0\"\n",
        "        image_name = f\"image_{idx}_predicted_{predicted}.png\"\n",
        "        image_path = os.path.join(prediction_folder, image_name)\n",
        "\n",
        "        plt.imshow(original_image, cmap='gray')\n",
        "        plt.title(f\"Predicted: {'road' if predicted == 1 else 'field'}\")\n",
        "        plt.savefig(image_path)\n",
        "        plt.clf()  # Clear the plot for the next iteration\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "In6rEiI72gZr",
        "outputId": "15ef5060-c224-4034-ff05-fdcaf4dd6c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 0: Predicted field\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 1: Predicted field\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 2: Predicted road\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 3: Predicted road\n",
            "Image 4: Predicted field\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 5: Predicted road\n",
            "Image 6: Predicted road\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 7: Predicted road\n",
            "Image 8: Predicted road\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 9: Predicted field\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/predicted_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxWk9fZIQ8d7",
        "outputId": "ac4d0c2e-9e92-4418-9cd7-bb1dc15e95a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "zip error: Nothing to do! (/content/predicted_0.zip)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r predicted_1.zip predicted_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlREBEYjRYn2",
        "outputId": "4ca3e81a-7ddd-478e-f260-4630e303837c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: predicted_1/ (stored 0%)\n",
            "  adding: predicted_1/image_7_predicted_1.png (deflated 1%)\n",
            "  adding: predicted_1/image_6_predicted_1.png (deflated 1%)\n",
            "  adding: predicted_1/image_5_predicted_1.png (deflated 2%)\n",
            "  adding: predicted_1/image_3_predicted_1.png (deflated 1%)\n",
            "  adding: predicted_1/image_2_predicted_1.png (deflated 1%)\n",
            "  adding: predicted_1/image_8_predicted_1.png (deflated 2%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "toRP8cydT62M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}